{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a847605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dfbee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_company_video_url(series_company_video_url):\n",
    "    lst_company_video_url = list(filter(lambda x: (x != \"Unknown\") and bool(x) and (not isinstance(x, float)),\n",
    "                                        series_company_video_url))\n",
    "    if len(lst_company_video_url):\n",
    "        return lst_company_video_url[0]\n",
    "    else:\n",
    "        return series_company_video_url.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3542a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_salary(x):\n",
    "    '''\n",
    "    x = 'Dưới50,8 Tr VND'\n",
    "    y = 'Dưới500,000 VND'\n",
    "    z = '9,2 Tr - 30,2 Tr VND'\n",
    "    t = 'Trên 17,6 Tr VND'\n",
    "    \n",
    "    processing_salary(y)\n",
    "    '''\n",
    "    pattern_so = r\"(^[\\d]+(,[\\d]+)? (Tr )?- [\\d]+(,[\\d]+)? (Tr )?VND)\"\n",
    "    pattern_duoi = r\"^Dưới[\\d]+(,[\\d]+)? (Tr )?VND\"\n",
    "    pattern_tren = r\"^Trên [\\d]+(,[\\d]+)? (Tr )?VND\"\n",
    "    if x == \"Cạnh tranh\":\n",
    "        return ['Unknown', 'Unknown']\n",
    "    elif bool(re.match(pattern_so, x)):\n",
    "        try:\n",
    "            re_match_obj = re.match(r\"^[\\d]+(,[\\d]+)? (Tr )?\", x)\n",
    "            idx_s_lb_salary = re_match_obj.start()\n",
    "            idx_e_lb_salary = re_match_obj.end()\n",
    "            lb_salary = x[idx_s_lb_salary:idx_e_lb_salary].strip()\n",
    "            lb_salary = float(lb_salary.replace(\",\", \"\"))/1_000_000 if (lb_salary[-3:]!=\" Tr\")\\\n",
    "                else float(lb_salary[:-3].replace(\",\", \".\"))\n",
    "            re_search_obj = re.search(r\"[\\d]+(,[\\d]+)? (Tr )?\", x[idx_e_lb_salary:])\n",
    "            idx_s_ub_salary = re_search_obj.start()\n",
    "            idx_e_ub_salary = re_search_obj.end()\n",
    "            ub_salary = x[idx_e_lb_salary+idx_s_ub_salary:idx_e_lb_salary+idx_e_ub_salary].strip()\n",
    "            ub_salary = float(ub_salary.replace(\",\", \"\"))/1_000_000 if (ub_salary[-3:]!=\" Tr\")\\\n",
    "                else float(ub_salary[:-3].replace(\",\", \".\"))\n",
    "            return [lb_salary, ub_salary]\n",
    "        except:\n",
    "            return [x, '']\n",
    "    elif bool(re.match(pattern_duoi, x)):\n",
    "        try:\n",
    "            re_search_obj = re.search(r\"[\\d]+(,[\\d]+)? (Tr )?\", x)\n",
    "            idx_s_ub_salary = re_search_obj.start()\n",
    "            idx_e_ub_salary = re_search_obj.end()\n",
    "            ub_salary = x[idx_s_ub_salary:idx_e_ub_salary].strip()\n",
    "            ub_salary = float(ub_salary.replace(\",\", \"\"))/1_000_000 if (ub_salary[-3:]!=\" Tr\")\\\n",
    "                else float(ub_salary[:-3].replace(\",\", \".\"))\n",
    "            lb_salary = ub_salary * 0.32\n",
    "            return [lb_salary, ub_salary]\n",
    "        except:\n",
    "            return [x, '']\n",
    "    elif bool(re.match(pattern_tren, x)):\n",
    "        try:\n",
    "            re_search_obj = re.search(r\"[\\d]+(,[\\d]+)? (Tr )?\", x)\n",
    "            idx_s_lb_salary = re_search_obj.start()\n",
    "            idx_e_lb_salary = re_search_obj.end()\n",
    "            lb_salary = x[idx_s_lb_salary:idx_e_lb_salary].strip()\n",
    "            lb_salary = float(lb_salary.replace(\",\", \"\"))/1_000_000 if (lb_salary[-3:]!=\" Tr\")\\\n",
    "                else float(lb_salary[:-3].replace(\",\", \".\"))\n",
    "            ub_salary = lb_salary * 1.4\n",
    "            return [lb_salary, ub_salary]\n",
    "        except:\n",
    "            return [x, '']\n",
    "    else:\n",
    "        return [x, '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96153d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_col_by_col(df, replaced_col, replacing_col, idx_repaired, idx_repaired_2):\n",
    "    print(f\"Unique values of \\\"{replacing_col}\\\" to repair\")\n",
    "    print(df.loc[idx_repaired_2, replacing_col].unique())\n",
    "    print()\n",
    "\n",
    "    print(f\"Unique values of \\\"{replaced_col}\\\" to be repaired\")\n",
    "    print(df.loc[idx_repaired_2, replaced_col].unique())\n",
    "    print()\n",
    "\n",
    "    print(\"Repairing\")\n",
    "    df.loc[idx_repaired_2, replaced_col] = df.loc[idx_repaired_2, replacing_col]\n",
    "    df.loc[idx_repaired_2, replacing_col] = \"Unknown\"\n",
    "    print()\n",
    "    print('Done')\n",
    "    print()\n",
    "\n",
    "    print(f\"Unique values of \\\"{replacing_col}\\\" to repair\")\n",
    "    print(df.loc[idx_repaired_2, replacing_col].unique())\n",
    "    print()\n",
    "\n",
    "    print(f\"Unique values of \\\"{replaced_col}\\\" to be repaired\")\n",
    "    print(df.loc[idx_repaired_2, replaced_col].unique())\n",
    "    print()\n",
    "\n",
    "    print(\"Reduce idx_repaired\")\n",
    "    idx_repaired = list(set(idx_repaired) - set(idx_repaired_2))\n",
    "    print(\"Done\")\n",
    "    print(\"Number of remaining error rows:\", len(idx_repaired))\n",
    "    print()\n",
    "    \n",
    "    return df, idx_repaired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af6af1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_kinh_nghiem(x):\n",
    "    \"\"\"\n",
    "    x = '10 - 11 Năm'\n",
    "    y = '10 Năm'\n",
    "    z = 'Dưới 10Năm'\n",
    "    t = 'Trên 12 Năm'\n",
    "    processing_kinh_nghiem(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    pattern_1 = r\"(^[\\d]+ - [\\d]+ Năm)\"     # ? - ? Năm\n",
    "    pattern_2 = r\"(^[\\d]+ Năm)\"             # ? Năm\n",
    "    pattern_3 = r\"(^Dưới [\\d]+Năm)\"         # Dưới ?Năm\n",
    "    pattern_4 = r\"(^Trên [\\d]+ Năm)\"        # Trên ? Năm\n",
    "    if x == \"Chưa có kinh nghiệm\":\n",
    "        return [0, 0]\n",
    "    elif bool(re.match(pattern_1, x)):\n",
    "        try:\n",
    "            re_match_obj = re.match(r\"^[\\d]+\", x)\n",
    "            idx_s_lb_kn = re_match_obj.start()\n",
    "            idx_e_lb_kn = re_match_obj.end()\n",
    "            lb_kn = x[idx_s_lb_kn:idx_e_lb_kn].strip()\n",
    "            lb_kn = int(lb_kn)\n",
    "            re_search_obj = re.search(r\"[\\d]+\", x[idx_e_lb_kn:])\n",
    "            idx_s_ub_kn = re_search_obj.start()\n",
    "            idx_e_ub_kn = re_search_obj.end()\n",
    "            ub_kn = x[idx_e_lb_kn+idx_s_ub_kn:idx_e_lb_kn+idx_e_ub_kn].strip()\n",
    "            ub_kn = int(ub_kn)\n",
    "            return [lb_kn, ub_kn]\n",
    "        except:\n",
    "            return [x, '']\n",
    "    elif bool(re.match(pattern_2, x)):\n",
    "        try:\n",
    "            re_match_obj = re.match(r\"^[\\d]+\", x)\n",
    "            idx_s_lb_kn = re_match_obj.start()\n",
    "            idx_e_lb_kn = re_match_obj.end()\n",
    "            lb_kn = x[idx_s_lb_kn:idx_e_lb_kn].strip()\n",
    "            lb_kn = int(lb_kn)\n",
    "            return [lb_kn, lb_kn]\n",
    "        except:\n",
    "            return [x, '']\n",
    "    elif bool(re.match(pattern_3, x)):\n",
    "        try:\n",
    "            re_match_obj = re.match(r\"^[\\d]+\", x[5:])\n",
    "            idx_s_ub_kn = re_match_obj.start()\n",
    "            idx_e_ub_kn = re_match_obj.end()\n",
    "            ub_kn = x[5+idx_s_ub_kn:5+idx_e_ub_kn].strip()\n",
    "            ub_kn = int(ub_kn)\n",
    "            return [0, ub_kn]\n",
    "        except:\n",
    "            return [x, '']\n",
    "    elif bool(re.match(pattern_4, x)):\n",
    "        try:\n",
    "            re_match_obj = re.match(r\"^[\\d]+\", x[5:])\n",
    "            idx_s_lb_kn = re_match_obj.start()\n",
    "            idx_e_lb_kn = re_match_obj.end()\n",
    "            lb_kn = x[5+idx_s_lb_kn:5+idx_e_lb_kn].strip()\n",
    "            lb_kn = int(lb_kn)\n",
    "            return [lb_kn, 40]\n",
    "        except:\n",
    "            return [x, '']\n",
    "    else:\n",
    "        return [x, '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a0be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_pipeline(link_to_data_files, link_to_output):\n",
    "    # link_to_data_files = r\"vn_data_jobs\\data cb\\raw data cb\\all_jobs\"\n",
    "    # link_to_output = r\"C:\\Users\\Admin\\Downloads\"\n",
    "    print(\"Query df_companies_sv from server:\")\n",
    "    df_companies_sv = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\DE 2nd folder\\DE project\\scrape component\\vn_data_jobs\\data cb\\cleansed data cb\\companies.csv\",\n",
    "                                  usecols=['company_title', 'company_id'], keep_default_na=False)\n",
    "    print(df_companies_sv)\n",
    "    print()\n",
    "\n",
    "    print(\"Number of company_id unique:\", df_companies_sv['company_id'].unique().size, \"unique values\")\n",
    "    print(\"Check number of company_id unique:\")\n",
    "    if len(df_companies_sv) == df_companies_sv['company_id'].unique().size:\n",
    "        print(True)\n",
    "    else:\n",
    "        raise Exception(\"Number of company_id unique does not equal to total number of rows of df_companies_sv\")\n",
    "    print()\n",
    "\n",
    "    print(\"Number of company_title unique:\", df_companies_sv['company_title'].unique().size, \"unique values\")\n",
    "    print(\"Check number of company_title unique:\")\n",
    "    if len(df_companies_sv) == df_companies_sv['company_title'].unique().size:\n",
    "        print(True)\n",
    "    else:\n",
    "        raise Exception(\"Number of company_title unique does not equal to total number of rows of df_companies_sv\")\n",
    "    print()\n",
    "\n",
    "    print(\"Query df_job_id from server:\")\n",
    "    df_job_id = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\DE 2nd folder\\DE project\\scrape component\\vn_data_jobs\\data cb\\cleansed data cb\\jobs.csv\",\n",
    "                                  usecols=['job_id'], keep_default_na=False)\n",
    "    print(df_job_id)\n",
    "    print()\n",
    "\n",
    "    print(\"Number of job_id unique:\", df_job_id['job_id'].unique().size, \"unique values\")\n",
    "    print(\"Check number of job_id unique:\")\n",
    "    if len(df_job_id) == df_job_id['job_id'].unique().size:\n",
    "        print(True)\n",
    "    else:\n",
    "        raise Exception(\"Number of job_id unique does not equal to total number of rows of df_job_id\")\n",
    "    print()\n",
    "\n",
    "    print(\"Query df_ordered_cap_bac from server:\")\n",
    "    df_ordered_cap_bac = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\DE 2nd folder\\DE project\\scrape component\\vn_data_jobs\\data cb\\cleansed data cb\\ordered_cap_bac.csv\",\n",
    "                                     keep_default_na=False)\n",
    "    print(df_ordered_cap_bac)\n",
    "    print()\n",
    "\n",
    "    cap_bac = list(filter(lambda x: x!=\"Unknown\", df_ordered_cap_bac['cap_bac'].unique()))\n",
    "\n",
    "    print(\"Query city from server:\")\n",
    "    df_city = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\DE 2nd folder\\DE project\\scrape component\\vn_data_jobs\\data cb\\cleansed data cb\\city_country.csv\",\n",
    "                                  usecols=['City'], keep_default_na=False)\n",
    "    print(df_city)\n",
    "    print()\n",
    "\n",
    "    print(\"Number of city unique:\", df_city['City'].unique().size, \"unique values\")\n",
    "    print(\"Check number of city unique:\")\n",
    "    if len(df_city) == df_city['City'].unique().size:\n",
    "        print(True)\n",
    "    else:\n",
    "        raise Exception(\"Number of city unique does not equal to total number of rows of df_city\")\n",
    "    print()\n",
    "\n",
    "    print(\"1) Processing raw data\")\n",
    "    print()\n",
    "\n",
    "    lst_data_files = os.listdir(link_to_data_files)\n",
    "    print(\"List raw data files:\")\n",
    "    for data_file in lst_data_files:\n",
    "        print(\"\\t\", data_file)\n",
    "    print()\n",
    "\n",
    "    print(\"1.1) Combine raw data\")\n",
    "    print()\n",
    "    total_rows = 0\n",
    "    df = pd.DataFrame()\n",
    "    for data_file in lst_data_files:\n",
    "        sub_df = pd.read_csv(os.path.join(link_to_data_files, data_file))\n",
    "        print(\"\\t\", data_file, \"-\", len(sub_df), \"rows\")\n",
    "        total_rows += len(sub_df)\n",
    "        df = pd.concat([df, sub_df], ignore_index=True)\n",
    "    del data_file\n",
    "    del sub_df\n",
    "    print()\n",
    "    print(\"Total rows:\", total_rows)\n",
    "    print()\n",
    "\n",
    "    df.info()\n",
    "    print()\n",
    "\n",
    "    print(\"Drop NaN job_id\")\n",
    "    print(f\"Total rows before drop: {len(df)}\")\n",
    "    df = df.dropna(subset=['job_id'])\n",
    "    print(\"Done\")\n",
    "    print(f\"Total rows before drop: {len(df)}\")\n",
    "    print()\n",
    "\n",
    "    print(\"Fill Unknown for all NaN values\")\n",
    "    df = df.fillna(\"Unknown\")\n",
    "    print(\"Done\")\n",
    "    print()\n",
    "    df.info()\n",
    "    print()\n",
    "\n",
    "    print(\"Strip all cell values\")\n",
    "    for col in df:\n",
    "        df[col] = df[col].map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    print(\"Done\")\n",
    "    print()\n",
    "\n",
    "    print(\"Check total rows:\")\n",
    "    if len(df) == total_rows:\n",
    "        print(True)\n",
    "    else:\n",
    "        raise Exception(\"Number of rows in combined_raw_data (df) does not equal to total_rows counted by summing from each raw data files\")\n",
    "\n",
    "    del total_rows\n",
    "    print()\n",
    "\n",
    "    # Assum has new raw data\n",
    "    print(\"Filter existence job_id:\")\n",
    "    df = df[~df['job_id'].isin(df_job_id['job_id'])]\n",
    "    print(\"Done\")\n",
    "    print()\n",
    "    df.info()\n",
    "    print()\n",
    "\n",
    "    # ======================================\n",
    "    # ==================================\n",
    "    # Assum has new raw data\n",
    "    # ==================================\n",
    "    # ======================================\n",
    "\n",
    "    if len(df):\n",
    "        print(\"Number of job_id unique:\", df['job_id'].unique().size, \"unique values\")\n",
    "        print(\"Check number of job_id unique:\")\n",
    "        if len(df) == df['job_id'].unique().size:\n",
    "            print(True)\n",
    "        else:\n",
    "            raise Exception(\"Number of rows in combined_raw_data (df) does not equal to number of job_id unique\")\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(\"1.2) Processing \\\"location\\\" column\")\n",
    "        print()\n",
    "\n",
    "        print(\"Cities in raw data:\")\n",
    "        for city in set(reduce(lambda x, y: x+y, df['location'].map(lambda x: list(map(lambda y: y.strip(), x.split(\" | \")))).values)):\n",
    "            print(\"  '\" + city + \"'\")\n",
    "        print()\n",
    "\n",
    "        print(\"Process \\\"location\\\" column\")\n",
    "        df['location'] = df['location'].map(lambda x: list(map(lambda y: y.strip(), x.split(\" | \"))))\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "\n",
    "        print('Explode location_job dataset')\n",
    "        df_jobs = df[['location', 'job_id', 'company_title']].explode('location')\\\n",
    "            .rename(columns={\"location\": \"City\"})\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        print(df_jobs)\n",
    "        print()\n",
    "\n",
    "        print(\"Total number of cities in raw data:\", len(reduce(lambda x, y: x+y, df['location'].values)), \"cities\")\n",
    "        print(\"Check total number of cities in raw data:\")\n",
    "        if len(df_jobs) == len(reduce(lambda x, y: x+y, df['location'].values)):\n",
    "            print(True)\n",
    "        else:\n",
    "            raise Exception(\"Number of rows in df_jobs does not equal to total number of cities in raw data\")\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(\"Fill NaN values with \\\"Unknown\\\"\")\n",
    "        df_jobs = df_jobs.fillna('Unknown')\n",
    "        print()\n",
    "\n",
    "        print(\"Check drop_duplicates makes difference\")\n",
    "        if len(df_jobs) == len(df_jobs.drop_duplicates()):\n",
    "            print(True)\n",
    "        else:\n",
    "            raise Exception(\"Number of rows in df_jobs does not equal to number of rows in df_jobs.drop_duplicates()\")\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(\"Check for existence of company_title in raw data (Any new company title?)\")\n",
    "        df_jobs_unknown = df_jobs[~df_jobs['company_title'].isin(df_companies_sv['company_title'].values)]\n",
    "        print(df_jobs_unknown)\n",
    "        print()\n",
    "\n",
    "        if len(df_jobs_unknown):\n",
    "            print(\"There were some companies that their titles do not exists in database (df_companies_sv['company_title']) before\")\n",
    "            df_jobs_unknown = df_jobs_unknown[['company_title']].drop_duplicates()\n",
    "            print()\n",
    "            print(df_jobs_unknown)\n",
    "            print()\n",
    "\n",
    "            print(\"Number of companies needs to add to database:\", len(df_jobs_unknown))\n",
    "            print()\n",
    "\n",
    "            print(\"df_companies_sv\")\n",
    "            print(df_companies_sv)\n",
    "            print()\n",
    "\n",
    "            print(\"Add company_id for new companies\")\n",
    "            max_value_company_id = df_companies_sv['company_id'].map(lambda x: int(x[1:])).max()\n",
    "            df_jobs_unknown['company_id'] = range(max_value_company_id+1, max_value_company_id+1+len(df_jobs_unknown))\n",
    "            df_jobs_unknown['company_id'] = df_jobs_unknown['company_id'].map(lambda x: \"C\" + \"0\"*(5-len(str(x))) + str(x))\n",
    "            df_jobs_unknown = df_jobs_unknown[['company_id', 'company_title']]\n",
    "            print(\"Done\")\n",
    "            print()\n",
    "            print(df_jobs_unknown)\n",
    "            print()\n",
    "\n",
    "            print(\"Concatenate df_jobs_unknown to df_companies_sv\")\n",
    "            df_companies_sv = pd.concat([df_companies_sv, df_jobs_unknown], ignore_index=False)\n",
    "            print(\"Done\")\n",
    "            print()\n",
    "            print(df_companies_sv)\n",
    "            print()\n",
    "\n",
    "            print(\"Get additional columns: \\\"company_url\\\" and \\\"company_video_url\\\" columns\")\n",
    "            df_jobs_unknown = df_jobs_unknown.merge(df[['company_title', 'company_url', 'company_video_url']], on='company_title',\n",
    "                                                    how='left')\n",
    "            print()\n",
    "            print(df_jobs_unknown)\n",
    "            print()\n",
    "\n",
    "            print(\"Processing \\\"company_url\\\" and \\\"company_video_url\\\" columns to get unique \\\"company_id\\\" and \\\"company_title\\\" columns\")\n",
    "            df_jobs_unknown = df_jobs_unknown.groupby(['company_id'], as_index=False).agg({\"company_title\": \"first\",\n",
    "                                                                         \"company_url\": filter_company_video_url,\n",
    "                                                                         \"company_video_url\": filter_company_video_url})\n",
    "            df_jobs_unknown = df_jobs_unknown.fillna(\"Unknown\")\n",
    "            print()\n",
    "            print(df_jobs_unknown)\n",
    "            print()\n",
    "        else:\n",
    "            df_jobs_unknown = pd.DataFrame({\"company_id\": [], \"company_title\": [], \"company_url\": [], \"company_video_url\": [], })\n",
    "\n",
    "        print(\"Save df_jobs_unknown to \\\"companies.csv\\\"\")\n",
    "        df_jobs_unknown.to_csv(os.path.join(link_to_output, r\"companies.csv\"), index=False)\n",
    "        print()\n",
    "\n",
    "        print(\"df_jobs merge with df_companies_sv\")\n",
    "        df_jobs = df_jobs.merge(df_companies_sv, on='company_title', how='left')\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        print(df_jobs)\n",
    "        print()\n",
    "\n",
    "        # Processing df_jobs is done -> save location_job.csv\n",
    "        print(\"Save df_jobs to \\\"location_job.csv\\\"\")\n",
    "        df_jobs = df_jobs.fillna(\"Unknown\")\n",
    "        df_jobs.to_csv(os.path.join(link_to_output, r\"location_job.csv\"), index=False)\n",
    "        print()\n",
    "\n",
    "        print(\"Find unknown cities\")\n",
    "        set_unknown_cities = set(df_jobs['City'].unique()) - set(df_city['City'].unique())\n",
    "        print(set_unknown_cities)\n",
    "        print()\n",
    "\n",
    "        print(\"Save unknown_cities to \\\"city_country.csv\\\"\")\n",
    "        pd.DataFrame({\"City\": list(set_unknown_cities), \"Country\": [np.NaN for i in range(len(list(set_unknown_cities)))]})\\\n",
    "            .to_csv(os.path.join(link_to_output, r\"city_country.csv\"), index=False)\n",
    "        print()\n",
    "\n",
    "        print(\"Merge with df_companies_sv\")\n",
    "        df = df.merge(df_companies_sv, on='company_title',how='left')\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        df.info()\n",
    "        print()\n",
    "\n",
    "        print(\"Drop \\\"location\\\", \\\"company_title\\\", \\\"company_url\\\" and \\\"company_video_url\\\" columns\")\n",
    "        df = df.drop(['location', 'company_title', 'company_url', 'company_video_url'], axis=1)\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        df.info()\n",
    "        print()\n",
    "\n",
    "        print(\"1.3) Processing \\\"outstanding_welfare\\\" column\")\n",
    "        print()\n",
    "\n",
    "        print(\"outstanding_welfare in raw data:\")\n",
    "        for outstanding_welfare in set(reduce(lambda x, y: x+y, df['outstanding_welfare'].map(lambda x: list(map(lambda y: y.strip(), x.split(\" | \")))).values)):\n",
    "            print(\"  '\" + outstanding_welfare + \"'\")\n",
    "        print()\n",
    "\n",
    "        print(\"Repair wrong text\")\n",
    "        df['outstanding_welfare'] = df['outstanding_welfare'].map(lambda x: x.replace(\"hiểểm\", \"hiểm\"))\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "\n",
    "        print(\"outstanding_welfare in raw data:\")\n",
    "        for outstanding_welfare in set(reduce(lambda x, y: x+y, df['outstanding_welfare'].map(lambda x: list(map(lambda y: y.strip(), x.split(\" | \")))).values)):\n",
    "            print(\"  '\" + outstanding_welfare + \"'\")\n",
    "        print()\n",
    "\n",
    "        print(\"Processing \\\"outstanding_welfare\\\" column\")\n",
    "        df['outstanding_welfare'] = df['outstanding_welfare'].map(lambda x: list(map(lambda y: y.strip(), x.split(\" | \"))))\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "\n",
    "        print('Explode df_outstanding_welfare dataset')\n",
    "        df_outstanding_welfare = df[['job_id', 'outstanding_welfare']].explode('outstanding_welfare')\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        print(df_outstanding_welfare)\n",
    "        print()\n",
    "\n",
    "        print(\"Save df_outstanding_welfare to \\\"outstanding_welfare_job.csv\\\"\")\n",
    "        df_outstanding_welfare.to_csv(os.path.join(link_to_output, r\"outstanding_welfare_job.csv\"), index=False)\n",
    "        print()\n",
    "\n",
    "        print(\"Drop \\\"outstanding_welfare\\\" column\")\n",
    "        df = df.drop(['outstanding_welfare'], axis=1)\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        df.info()\n",
    "        print()\n",
    "\n",
    "        print(\"1.4) Processing \\\"detailed_welfare\\\" column\")\n",
    "        print()\n",
    "\n",
    "        print(\"detailed_welfare in raw data:\")\n",
    "        for detailed_welfare in set(reduce(lambda x, y: x+y, df['detailed_welfare'].map(lambda x: list(map(lambda y: y.strip(), x.split(\" | \")))).values)):\n",
    "            print(\"  '\" + detailed_welfare + \"'\")\n",
    "        print()\n",
    "\n",
    "        print(\"Processing \\\"detailed_welfare\\\" column\")\n",
    "        df['detailed_welfare'] = df['detailed_welfare'].map(lambda x: list(map(lambda y: y.strip(), x.split(\" | \"))))\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "\n",
    "        print('Explode df_detailed_welfare dataset')\n",
    "        df_detailed_welfare = df[['job_id', 'detailed_welfare']].explode('detailed_welfare')\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        print(df_detailed_welfare)\n",
    "        print()\n",
    "\n",
    "        print(\"Save df_detailed_welfare to \\\"detailed_welfare_job.csv\\\"\")\n",
    "        df_detailed_welfare.to_csv(os.path.join(link_to_output, r\"detailed_welfare_job.csv\"), index=False)\n",
    "        print()\n",
    "\n",
    "        print(\"Drop \\\"detailed_welfare\\\" column\")\n",
    "        df = df.drop(['detailed_welfare'], axis=1)\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        df.info()\n",
    "        print()\n",
    "\n",
    "        print(\"1.5) Processing \\\"nganh_nghe\\\" column\")\n",
    "        print()\n",
    "\n",
    "        print(\"nganh_nghe in raw data:\")\n",
    "        for nganh_nghe in set(reduce(lambda x, y: x+y, df['nganh_nghe'].map(lambda x: list(map(lambda y: y.strip(), x.split(\" | \")))).values)):\n",
    "            print(\"  '\" + nganh_nghe + \"'\")\n",
    "        print()\n",
    "\n",
    "        print(\"Processing \\\"nganh_nghe\\\" column\")\n",
    "        df['nganh_nghe'] = df['nganh_nghe'].map(lambda x: list(map(lambda y: y.strip(), x.split(\" | \"))))\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "\n",
    "        print('Explode df_nganh_nghe dataset')\n",
    "        df_nganh_nghe = df[['job_id', 'nganh_nghe']].explode('nganh_nghe')\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        print(df_nganh_nghe)\n",
    "        print()\n",
    "\n",
    "        print(\"Save df_nganh_nghe to \\\"nganh_nghe_job.csv\\\"\")\n",
    "        df_nganh_nghe.to_csv(os.path.join(link_to_output, r\"nganh_nghe_job.csv\"), index=False)\n",
    "        print()\n",
    "\n",
    "        print(\"Drop \\\"nganh_nghe\\\" column\")\n",
    "        df = df.drop(['nganh_nghe'], axis=1)\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        df.info()\n",
    "        print()\n",
    "\n",
    "        print(\"1.6) Processing \\\"job_tags\\\" column\")\n",
    "        print()\n",
    "\n",
    "        print(\"job_tags in raw data:\")\n",
    "        for job_tags in set(reduce(lambda x, y: x+y, df['job_tags'].map(lambda x: list(map(lambda y: y.strip(), x.split(\" | \")))).values)):\n",
    "            print(\"  '\" + job_tags + \"'\")\n",
    "        print()\n",
    "\n",
    "        print(\"Processing \\\"job_tags\\\" column\")\n",
    "        df['job_tags'] = df['job_tags'].map(lambda x: list(map(lambda y: y.strip(), x.split(\" | \"))))\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "\n",
    "        print('Explode df_job_tags dataset')\n",
    "        df_job_tags = df[['job_id', 'job_tags']].explode('job_tags')\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        print(df_job_tags)\n",
    "        print()\n",
    "\n",
    "        print(\"Save df_job_tags to \\\"job_tags_job.csv\\\"\")\n",
    "        df_job_tags.to_csv(os.path.join(link_to_output, r\"job_tags_job.csv\"), index=False)\n",
    "        print()\n",
    "\n",
    "        print(\"Drop \\\"job_tags\\\" column\")\n",
    "        df = df.drop(['job_tags'], axis=1)\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        df.info()\n",
    "        print()\n",
    "\n",
    "        print(\"1.7) Processing \\\"salary\\\" column\")\n",
    "        print()\n",
    "\n",
    "        print(\"Test if all values in \\\"saraly\\\" column start with 'Lương: '\")\n",
    "        print(df[df['salary'].map(lambda x: x[:7]!='Lương: ')])\n",
    "        print()\n",
    "\n",
    "        print(\"Test if remove 7 characters at beginning (x[7:]), and get unique values\")\n",
    "        for i in sorted(df['salary'].map(lambda x: x[7:]).unique()):\n",
    "            print(i)\n",
    "        print()\n",
    "\n",
    "        # Số...\n",
    "        # Cạnh tranh\n",
    "        # Dưới...\n",
    "        # Trên...\n",
    "        print(\"Processing \\\"salary\\\" column: remove 7 characters at beginning (x[7:])\")\n",
    "        df['salary'] = df['salary'].map(lambda x: x[7:])\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        print(df['salary'])\n",
    "        print()\n",
    "\n",
    "        print(\"Check after removing all known patterns\")\n",
    "        for i in sorted(df[df['salary'].map(lambda x: not (bool(re.match(r\"(^[\\d]+(,[\\d]+)? Tr - [\\d]+(,[\\d]+)? Tr VND)\", x))\n",
    "                                                        or bool(re.match(r\"^Dưới[\\d]+(,[\\d]+)? (Tr )?VND\", x))\n",
    "                                                        or bool(re.match(r\"^Trên [\\d]+(,[\\d]+)? (Tr )?VND\", x))))]['salary'].unique()):\n",
    "            print(\" '\" + i + \"'\")\n",
    "\n",
    "        print(\"Processing \\\"salary\\\" column: processing with processing_salary function\")\n",
    "        df['salary'] = df['salary'].map(processing_salary)\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "\n",
    "        print(\"Split \\\"salary\\\" column into two columns: \\\"lb_salary\\\" column and \\\"ub_salary\\\" columns\")\n",
    "        df['lb_salary'] = df['salary'].map(lambda x: x[0])\n",
    "        df['ub_salary'] = df['salary'].map(lambda x: x[1])\n",
    "\n",
    "        print(\"Drop \\\"salary\\\" column\")\n",
    "        df = df.drop(['salary'], axis=1)\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        df.info()\n",
    "        print()\n",
    "\n",
    "        print(\"1.8) Processing \\\"announcement_date\\\" column\")\n",
    "        print()\n",
    "\n",
    "        print(\"Check min and max of \\\"announcement_date\\\" column\")\n",
    "        print(\"Min:\", df['announcement_date'].map(len).min())\n",
    "        print(\"Max:\", df['announcement_date'].map(len).max())\n",
    "\n",
    "        print(\"Processing \\\"announcement_date\\\" column: rearrange date format YYYY-MM-DD\")\n",
    "        df['announcement_date'] = df['announcement_date'].map(lambda x: x[-4:] + \"-\" + x[3:5] + \"-\" + x[:2])\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "\n",
    "        print(df['announcement_date'])\n",
    "        print()\n",
    "\n",
    "        print(\"1.9) Processing \\\"hinh_thuc\\\" column\")\n",
    "        print()\n",
    "\n",
    "        print('Check unique values in \\\"hinh_thuc\\\" column')\n",
    "        print(df['hinh_thuc'].unique())\n",
    "        print()\n",
    "\n",
    "        print(\"hinh_thuc in raw data:\")\n",
    "        for hinh_thuc in set(reduce(lambda x, y: x+y, df['hinh_thuc'].map(lambda x: list(map(lambda y: y.strip(), x.split(\", \")))).values)):\n",
    "            print(\"  '\" + hinh_thuc + \"'\")\n",
    "        print()\n",
    "\n",
    "        print(\"Processing \\\"hinh_thuc\\\" column\")\n",
    "        df['hinh_thuc'] = df['hinh_thuc'].map(lambda x: list(map(lambda y: y.strip(), x.split(\", \"))))\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "\n",
    "        print('Explode df_hinh_thuc dataset')\n",
    "        df_hinh_thuc = df[['job_id', 'hinh_thuc']].explode('hinh_thuc')\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        print(df_hinh_thuc)\n",
    "        print()\n",
    "\n",
    "        print(\"Save df_hinh_thuc to \\\"hinh_thuc_job.csv\\\"\")\n",
    "        df_hinh_thuc.to_csv(os.path.join(link_to_output, r\"hinh_thuc_job.csv\"), index=False)\n",
    "        print()\n",
    "\n",
    "        print(\"Drop \\\"hinh_thuc\\\" column\")\n",
    "        df = df.drop(['hinh_thuc'], axis=1)\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "        df.info()\n",
    "        print()\n",
    "\n",
    "        print(\"1.10) Processing \\\"expiration_date\\\" column\")\n",
    "        print()\n",
    "\n",
    "        print(\"Copying \\\"expiration_date\\\" column\")\n",
    "        df['expiration_date_2'] = df['expiration_date']\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "\n",
    "        print(\"Check \\\"expiration_date\\\" that does not have format DD/MM/YYYY\")\n",
    "        idx_repaired = df[df['expiration_date'].map(lambda x: not bool(re.match(r\"(^[\\d]{2}\\/[\\d]{2}\\/[\\d]{4}$)\", x)))]['expiration_date'].index\n",
    "        print(\"Number of error rows:\", len(idx_repaired))\n",
    "        print()\n",
    "\n",
    "        if len(idx_repaired):\n",
    "            print(\"Mode repairing \\\"expiration_date\\\"\")\n",
    "            print(\"Unique error values:\")\n",
    "            print(df.loc[idx_repaired, 'expiration_date'].unique())\n",
    "            print()\n",
    "\n",
    "            print(\"Try to repair from \\\"kinh_nghiem\\\"\")\n",
    "            df_2 = df.loc[idx_repaired, ['kinh_nghiem']]\n",
    "            print()\n",
    "            print(df_2)\n",
    "            print()\n",
    "\n",
    "            print(\"Check if any \\\"kinh_nghiem\\\" have format of DD/MM/YYYY\")\n",
    "            idx_repaired_2 = df_2[df_2['kinh_nghiem'].map(lambda x: bool(re.match(r\"(^[\\d]{2}\\/[\\d]{2}\\/[\\d]{4}$)\", x)))].index\n",
    "            print()\n",
    "            print(\"Number of satisfied kinh_nghiem rows:\", len(idx_repaired_2))\n",
    "            print()\n",
    "\n",
    "            if len(idx_repaired_2):\n",
    "                df, idx_repaired = replace_col_by_col(df, 'expiration_date', 'kinh_nghiem', idx_repaired, idx_repaired_2)\n",
    "            else:\n",
    "                print(\"No satisfied kinh_nghiem row to repair\")\n",
    "\n",
    "            if len(idx_repaired):\n",
    "                print(\"Try to repair from \\\"cap_bac\\\"\")\n",
    "                df_2 = df.loc[idx_repaired, ['cap_bac']]\n",
    "                print()\n",
    "                print(df_2)\n",
    "                print()\n",
    "\n",
    "                print(\"Check if any \\\"cap_bac\\\" have format of DD/MM/YYYY\")\n",
    "                idx_repaired_2 = df_2[df_2['cap_bac'].map(lambda x: bool(re.match(r\"(^[\\d]{2}\\/[\\d]{2}\\/[\\d]{4}$)\", x)))].index\n",
    "                print()\n",
    "                print(\"Number of satisfied cap_bac rows:\", len(idx_repaired_2))\n",
    "                print()\n",
    "\n",
    "                if len(idx_repaired_2):\n",
    "                    df, idx_repaired = replace_col_by_col(df, 'expiration_date', 'cap_bac', idx_repaired, idx_repaired_2)\n",
    "                else:\n",
    "                    print(\"No satisfied cap_bac row to repair\")\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            print(\"No error format in \\\"expiration_date\\\"\")\n",
    "\n",
    "        print(\"Check min max of \\\"expiration_date\\\" column\")\n",
    "        print(\"Min:\", df['expiration_date'].map(len).min())\n",
    "        print(\"Max:\", df['expiration_date'].map(len).max())\n",
    "        print()\n",
    "\n",
    "        print(\"Check len not 10 in \\\"expiration_date\\\" column\")\n",
    "        print(df[df['expiration_date'].map(lambda x: len(x) != 10)]['expiration_date'].unique())\n",
    "        print()\n",
    "\n",
    "        print(\"Check len 10 in \\\"expiration_date\\\" column\")\n",
    "        print(df[df['expiration_date'].map(lambda x: len(x) == 10)]['expiration_date'].unique())\n",
    "        print()\n",
    "\n",
    "        print(\"Processing \\\"expiration_date\\\" column:\")\n",
    "        df['expiration_date'] = df['expiration_date'].map(lambda x: (x[-4:] + \"-\" + x[3:5] + \"-\" + x[:2]) if (x!=\"Unknown\") else x)\n",
    "        print(\"Done\")\n",
    "        print()\n",
    "\n",
    "        print(df['expiration_date'].unique())\n",
    "        print()\n",
    "\n",
    "        print(\"1.11) Processing \\\"cap_bac\\\" column\")\n",
    "        print()\n",
    "\n",
    "        print(\"Existing cap_bac in server\")\n",
    "        for i in cap_bac:\n",
    "            print(\" '\" + i + \"'\")\n",
    "        print()\n",
    "\n",
    "        print(\"Raw cap_bac in raw data\")\n",
    "        raw_cap_bac = list(filter(lambda x: x!=\"Unknown\", df['cap_bac'].unique()))\n",
    "        for i in raw_cap_bac:\n",
    "            print(\" '\" + i + \"'\")\n",
    "        print()\n",
    "\n",
    "        print(\"Check Unknown \\\"cap_bac\\\"\")\n",
    "        idx_repaired = df[df['cap_bac']=='Unknown'].index\n",
    "        print(\"Number of unknown \\\"cap_bac\\\":\", len(idx_repaired))\n",
    "        print()\n",
    "\n",
    "        if len(idx_repaired):\n",
    "            print(\"Mode repairing \\\"cap_bac\\\"\")\n",
    "            print(\"Try to repair from \\\"kinh_nghiem\\\"\")\n",
    "            df_2 = df.loc[idx_repaired, ['kinh_nghiem']]\n",
    "            print()\n",
    "            print(df_2)\n",
    "            print()\n",
    "\n",
    "            print(\"Check if any \\\"kinh_nghiem\\\" in list of raw_cap_bac\")\n",
    "            idx_repaired_2 = df_2[df_2['kinh_nghiem'].isin(raw_cap_bac)].index\n",
    "            print()\n",
    "            print(\"Number of satisfied kinh_nghiem rows:\", len(idx_repaired_2))\n",
    "            print()\n",
    "\n",
    "            if len(idx_repaired_2):\n",
    "                df, idx_repaired = replace_col_by_col(df, 'cap_bac', 'kinh_nghiem', idx_repaired, idx_repaired_2)\n",
    "            else:\n",
    "                print(\"No satisfied kinh_nghiem row to repair\")\n",
    "\n",
    "            if len(idx_repaired):\n",
    "                print(\"Try to repair from \\\"expiration_date_2\\\"\")\n",
    "                df_2 = df.loc[idx_repaired, ['expiration_date_2']]\n",
    "                print()\n",
    "                print(df_2)\n",
    "                print()\n",
    "\n",
    "                print(\"Check if any \\\"expiration_date_2\\\" in list of raw_cap_bac\")\n",
    "                idx_repaired_2 = df_2[df_2['expiration_date_2'].isin(raw_cap_bac)].index\n",
    "                print()\n",
    "                print(\"Number of satisfied expiration_date_2 rows:\", len(idx_repaired_2))\n",
    "                print()\n",
    "\n",
    "                if len(idx_repaired_2):\n",
    "                    df, idx_repaired = replace_col_by_col(df, 'cap_bac', 'expiration_date_2', idx_repaired, idx_repaired_2)\n",
    "                else:\n",
    "                    print(\"No satisfied expiration_date_2 row to repair\")\n",
    "            else:\n",
    "                print(\"No unknown \\\"cap_bac\\\"\")\n",
    "        else:\n",
    "            print(\"No unknown \\\"cap_bac\\\"\")\n",
    "\n",
    "        print(\"Check unique values in \\\"cap_bac\\\"\")\n",
    "        print(df['cap_bac'].unique())\n",
    "        print()\n",
    "\n",
    "        print(\"cap_bac:\", cap_bac)\n",
    "        print()\n",
    "\n",
    "        print(\"raw_cap_bac:\", raw_cap_bac)\n",
    "        print()\n",
    "\n",
    "        print(\"Check new cap_bac\")\n",
    "        new_cap_bac = list(set(raw_cap_bac) - set(cap_bac))\n",
    "        print(new_cap_bac)\n",
    "        print()\n",
    "\n",
    "        if len(new_cap_bac):\n",
    "            print(\"Some new_cap_bac exists\")\n",
    "            print(\"ordered_cap_bac in server\")\n",
    "            print(df_ordered_cap_bac)\n",
    "            print()\n",
    "\n",
    "            print(\"Create new ordered_cap_bac\")\n",
    "            df_ordered_cap_bac_new = pd.DataFrame({\"cap_bac\": new_cap_bac})\n",
    "            print(df_ordered_cap_bac_new)\n",
    "            print()\n",
    "\n",
    "            print(\"Create STT for new cap_bac\")\n",
    "            df_ordered_cap_bac_new['STT'] = range(len(df_ordered_cap_bac)+1, len(df_ordered_cap_bac)+1+len(new_cap_bac))\n",
    "            print(df_ordered_cap_bac_new)\n",
    "            print()\n",
    "        else:\n",
    "            print(\"There is no new_cap_bac\")\n",
    "            print(\"Create empty new ordered_cap_bac\")\n",
    "            df_ordered_cap_bac_new = pd.DataFrame({\"cap_bac\": [], \"STT\": []})\n",
    "            print(df_ordered_cap_bac_new)\n",
    "            print()\n",
    "\n",
    "        print(\"Save df_ordered_cap_bac_new to \\\"ordered_cap_bac.csv\\\"\")\n",
    "        df_ordered_cap_bac_new.to_csv(os.path.join(link_to_output, r\"ordered_cap_bac.csv\"), index=False)\n",
    "        print()\n",
    "\n",
    "        print(\"1.12) Processing \\\"kinh_nghiem\\\" column\")\n",
    "        print()\n",
    "\n",
    "        print(\"Unique values in \\\"kinh_nghiem\\\" column\")\n",
    "        for i in sorted(df['kinh_nghiem'].unique()):\n",
    "            print(\" '\" + i + \"'\")\n",
    "\n",
    "        print(\"Check after removing all known patterns\")\n",
    "        for i in sorted(df[df['kinh_nghiem'].map(lambda x: not (bool(re.match(r\"(^[\\d]+ - [\\d]+ Năm)\", x))\n",
    "                                                         or bool(re.match(r\"(^[\\d]+ Năm)\", x))\n",
    "                                                         or bool(re.match(r\"(^Dưới [\\d]+Năm)\", x))\n",
    "                                                         or bool(re.match(r\"(^Trên [\\d]+ Năm)\", x))))]['kinh_nghiem'].unique()):\n",
    "            print(\" '\" + i + \"'\")\n",
    "\n",
    "        print(\"Processing \\\"kinh_nghiem\\\" column\")\n",
    "        df['kinh_nghiem'] = df['kinh_nghiem'].map(processing_kinh_nghiem)\n",
    "        print(df['kinh_nghiem'])\n",
    "        print()\n",
    "\n",
    "        print(\"Split \\\"kinh_nghiem\\\" column into two columns: \\\"lb_kinh_nghiem\\\" column and \\\"ub_kinh_nghiem\\\" column\")\n",
    "        df['lb_kinh_nghiem'] = df['kinh_nghiem'].map(lambda x: x[0])\n",
    "        df['ub_kinh_nghiem'] = df['kinh_nghiem'].map(lambda x: x[1])\n",
    "        print()\n",
    "\n",
    "        print(\"Drop \\\"kinh_nghiem\\\" column\")\n",
    "        df = df.drop(['kinh_nghiem'], axis=1)\n",
    "\n",
    "        df.info()\n",
    "        print()\n",
    "\n",
    "        print(\"Drop \\\"expiration_date_2\\\" column\")\n",
    "        df = df.drop(['expiration_date_2'], axis=1)\n",
    "        df.info()\n",
    "        print()\n",
    "\n",
    "        print(\"Check fill Unknown for \\\"lb_kinh_nghiem\\\" column and \\\"ub_kinh_nghiem\\\" column\")\n",
    "        idx_repaired = df[df['ub_kinh_nghiem'].map(lambda x: True if not isinstance(x, int) else False)].index\n",
    "        print(\"Number of unknown kinh nghiem:\", len(idx_repaired))\n",
    "        print()\n",
    "\n",
    "        if len(idx_repaired):\n",
    "            print(\"Unique unknown values in \\\"lb_kinh_nghiem\\\" column\")\n",
    "            print(df.loc[idx_repaired]['lb_kinh_nghiem'].unique())\n",
    "            print()\n",
    "            print(\"Unique unknown values in \\\"ub_kinh_nghiem\\\" column\")\n",
    "            print(df.loc[idx_repaired]['ub_kinh_nghiem'].unique())\n",
    "            print()\n",
    "\n",
    "            print(\"Fill Unknown for \\\"lb_kinh_nghiem\\\" column and \\\"ub_kinh_nghiem\\\" column\")\n",
    "            df.loc[idx_repaired, \"lb_kinh_nghiem\"] = \"Unknown\"\n",
    "            df.loc[idx_repaired, \"ub_kinh_nghiem\"] = \"Unknown\"\n",
    "            print(\"Done\")\n",
    "            print()\n",
    "        else:\n",
    "            print(\"There is no unknown kinh nghiem\")\n",
    "    else:\n",
    "        print(\"All job_id in raw data have existed in server.\")\n",
    "        print(\" -> No processing pipeline needed.\")\n",
    "\n",
    "    print(\"Summary warning\")\n",
    "    if len(set_unknown_cities):\n",
    "        print(f\"Exists {len(set_unknown_cities)} new cities: {set_unknown_cities}\")\n",
    "    if len(new_cap_bac):\n",
    "        print(f\"Exists {len(new_cap_bac)} new cap_bac: {new_cap_bac}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
